AI is everywhere. Right from the moment one wakes up to unlock their phone using Face ID, to finding
the best route for commuting to their workplace using Google Maps, from the search recommendations
on every search engine, to when, at the end of the day, one unwinds with a movie Netflix recommends.
One of the primary goals of the field of artificial intelligence(AI) is to produce fully autonomous agents
that interact with their environments to learn optimal behaviours, improving overtime through trial and
error. There is a necessity of crafting AI systems which can sense and react to the world around them,
to purely software-based agents, which can interact with natural language and multimedia.
It has permeated every aspect of our day-to-day lives, so thoroughly and unbeknownst to everyone,
that it is high time to start thinking and worrying about its implications on humanity, its repercussions
and the hold it has on humanity as a whole. This is when we start looking into the field of AI Safety,
also known as AI Alignment, which aims to steer AI systems towards their designersâ€™ intended goals
and interests. The primary goal of AI alignment is to enable the design of agents that optimise for the
right reward function. If an AI agent misinterprets the objective it is given it may not allow its operator
to alter the objective, maybe by changing the environment. If machine intelligence is not designed to
be transparent and understandable to humans, it could make decisions that are opaque to humans and
difficult to understand or reverse.
